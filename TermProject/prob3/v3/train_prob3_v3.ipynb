{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "train_prob3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auZnofLD9q-t",
        "colab_type": "text"
      },
      "source": [
        "## Initialize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYXri2279swj",
        "colab_type": "text"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1Xr_8TXA8Q9",
        "colab_type": "code",
        "outputId": "1baedf2b-bbc8-4a76-82f6-6164083823d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import gzip\n",
        "import math\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "from datetime import datetime\n",
        "from time import time\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2zMmlT-98Yh",
        "colab_type": "text"
      },
      "source": [
        "### Defining Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpLYY1WR9-P5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = '.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJpDoVUH9us3",
        "colab_type": "text"
      },
      "source": [
        "### Mounting File System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg4QjiBNBMgH",
        "colab_type": "code",
        "outputId": "b6daf3a7-95a0-477f-d0ae-f06bb1000c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = './drive/My Drive/CoE202TermProject'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgg_pXshA8RF",
        "colab_type": "text"
      },
      "source": [
        "### Loading Input Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOXi9akbwHZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with gzip.open(base_path + '/datasets/train.chunk.pickle', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "\n",
        "with gzip.open(base_path + '/datasets/train.image.feats.pickle', 'rb') as f:\n",
        "    train_images = pickle.load(f)\n",
        "\n",
        "train_features = train_data['features'][:]\n",
        "train_labels = train_data['labels'][:]\n",
        "\n",
        "\n",
        "with gzip.open(base_path + '/datasets/valid.chunk.pickle', 'rb') as f:\n",
        "    valid_data = pickle.load(f)\n",
        "\n",
        "with gzip.open(base_path + '/datasets/valid.image.feats.pickle', 'rb') as f:\n",
        "    valid_images = pickle.load(f)\n",
        "\n",
        "valid_features = valid_data['features'][:]\n",
        "valid_labels = valid_data['labels'][:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2EedvGE-Xq1",
        "colab_type": "text"
      },
      "source": [
        "## Defining Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5gMGUXo-dbE",
        "colab_type": "text"
      },
      "source": [
        "### Defining Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlaW3FIsA8RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 512\n",
        "lr = 1e-4\n",
        "n_epochs = 3\n",
        "embd_size = 1024\n",
        "drop_prob = 0.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAx0-U1y-jrN",
        "colab_type": "text"
      },
      "source": [
        "### Defining Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm5PktEY-lGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voca_size  = 100001\n",
        "valid_freq = 100\n",
        "valid_freq_epoch = 1\n",
        "seqlen_model = len(train_features[0][2])\n",
        "seqlen_product = len(train_features[0][5])\n",
        "seqlen_image = len(train_images[0])\n",
        "n_classes  = np.max(train_labels) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtxIYrsYA8RP",
        "colab_type": "text"
      },
      "source": [
        "### Placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6txbjX2tA8RR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_brands = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, ), name='brands'\n",
        ")\n",
        "\n",
        "input_makers = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, ), name='makers'\n",
        ")\n",
        "\n",
        "input_products = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, seqlen_product), name='input_products'\n",
        ")\n",
        "\n",
        "input_product_counts = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, seqlen_product), name='input_product_counts'\n",
        ")\n",
        "\n",
        "input_images = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, 2048), name='input_images'\n",
        ")\n",
        "\n",
        "labels = tf.placeholder(\n",
        "    dtype=tf.int32, shape=(None, ), name='labels'\n",
        ")\n",
        "\n",
        "is_train = tf.placeholder(\n",
        "    dtype=tf.bool, name='is_train'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZw_gtkeA8RT",
        "colab_type": "text"
      },
      "source": [
        "### Deep neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o6qR3E3A8RU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Activation, BatchNormalization, \\\n",
        "                                    Concatenate, Dense, Dot, Dropout, \\\n",
        "                                    Embedding, Flatten, Reshape\n",
        "\n",
        "# Defining Embedding Layer\n",
        "embd = Embedding(voca_size, embd_size, name='embd')\n",
        "\n",
        "# Processing Brands\n",
        "x_brands = Reshape((1, ))(input_brands)\n",
        "x_brands = embd(x_brands)\n",
        "x_brands = BatchNormalization()(x_brands, training=is_train)\n",
        "x_brands = Flatten()(x_brands)\n",
        "\n",
        "# Processing Makers\n",
        "x_makers = Reshape((1, ))(input_makers)\n",
        "x_makers = embd(x_makers)\n",
        "x_makers = BatchNormalization()(x_makers, training=is_train)\n",
        "x_makers = Flatten()(x_makers)\n",
        "\n",
        "# Processing Products\n",
        "x_products = embd(input_products)\n",
        "x_product_counts = Reshape((seqlen_product, 1))(input_product_counts)\n",
        "x_products = Dot(1)([x_products, x_product_counts])\n",
        "x_products = Reshape((1, embd_size))(x_products)\n",
        "x_products = BatchNormalization()(x_products, training=is_train)\n",
        "x_products = Flatten()(x_products)\n",
        "\n",
        "texts = Concatenate(axis=1)([x_products, x_brands, x_makers])\n",
        "text_hidden = Dense(n_classes)(texts)\n",
        "text_hidden = BatchNormalization()(text_hidden, training=is_train)\n",
        "text_hidden = Dropout(drop_prob)(text_hidden, training=is_train)\n",
        "text_hidden = Activation('relu')(text_hidden)\n",
        "\n",
        "# Processing Images\n",
        "image_hidden = Dense(512)(input_images)\n",
        "image_hidden = BatchNormalization()(image_hidden, training=is_train)\n",
        "image_hidden = Dropout(drop_prob)(image_hidden, training=is_train)\n",
        "image_hidden = Activation('relu')(image_hidden)\n",
        "\n",
        "# Concatenating Images and Products\n",
        "x_concat = Concatenate(axis=1)([text_hidden, image_hidden])\n",
        "\n",
        "# Hidden Layer\n",
        "hidden1 = Dense(n_classes + n_classes // 3)(x_concat)\n",
        "hidden1 = BatchNormalization()(hidden1, training=is_train)\n",
        "hidden1 = Activation('relu')(hidden1)\n",
        "\n",
        "# Hidden Layer 2\n",
        "hidden2 = Dense(n_classes + n_classes // 3)(hidden1)\n",
        "hidden2 = BatchNormalization()(hidden2, training=is_train)\n",
        "hidden2 = Dropout(drop_prob)(hidden2, training=is_train)\n",
        "hidden2 = Activation('relu')(hidden2)\n",
        "\n",
        "# Output\n",
        "logits = Dense(n_classes)(hidden2)\n",
        "\n",
        "# Softmax\n",
        "preds = Activation('softmax', name='predictions')(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dye2seUzA8RX",
        "colab_type": "text"
      },
      "source": [
        "### Loss funciton & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSTWGvtlA8RZ",
        "colab_type": "code",
        "outputId": "f05d4878-5dcb-42f3-fa9f-97f19db39f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Softmax cross entropy loss\n",
        "loss = tf.losses.softmax_cross_entropy(onehot_labels=tf.one_hot(labels, n_classes), logits=logits)\n",
        "\n",
        "# Weight decay\n",
        "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "loss = tf.add_n([loss] + reg_losses, name='total_loss')\n",
        "\n",
        "# Optimizer\n",
        "optm = tf.train.AdamOptimizer(lr)\n",
        "train_op = optm.minimize(loss, global_step=tf.train.get_global_step(), name='step_update')\n",
        "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "train_op = tf.group([train_op, update_ops])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H7eVZH7G_ij",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9PfnmlmG_5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top1_acc = tf.keras.metrics.top_k_categorical_accuracy(y_true=tf.one_hot(labels, n_classes),\n",
        "                                                        y_pred=preds, k=1)\n",
        "top1_acc = tf.identity(top1_acc, name='top1_acc')\n",
        "\n",
        "top5_acc = tf.keras.metrics.top_k_categorical_accuracy(y_true=tf.one_hot(labels, n_classes),\n",
        "                                                        y_pred=preds, k=5)\n",
        "top5_acc = tf.identity(top5_acc, name='top5_acc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkUB1YJpA8Rb",
        "colab_type": "text"
      },
      "source": [
        "### Batch generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dypzaB9A8Rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(mode='training', use_percentage = 1):\n",
        "    if mode == 'training':\n",
        "        features = train_features\n",
        "        labels = train_labels\n",
        "        images = train_images\n",
        "\n",
        "    elif mode == 'valid':\n",
        "        features = valid_features\n",
        "        labels = valid_labels\n",
        "        images = valid_images\n",
        "    \n",
        "    elif mode == 'test':\n",
        "        features = test_features\n",
        "        labels = None\n",
        "        images = test_images\n",
        "\n",
        "    n_data = math.floor(len(features) * use_percentage)\n",
        "\n",
        "    indices = np.arange(len(features))\n",
        "    np.random.shuffle(indices)\n",
        "    indices = indices[:n_data]\n",
        "    \n",
        "    for start_idx in range(0, n_data, batch_size):\n",
        "        if start_idx + batch_size <= n_data:\n",
        "            excerpt = indices[start_idx: start_idx + batch_size]\n",
        "\n",
        "            if labels is not None:\n",
        "                yield list(zip(*features[excerpt, :])), \\\n",
        "                    images[excerpt, :], \\\n",
        "                    labels[excerpt]\n",
        "            \n",
        "            else:\n",
        "                yield list(zip(*features[excerpt, :])), \\\n",
        "                    images[excerpt, :]\n",
        "\n",
        "def get_feed_dict(b_features, b_images, b_label, b_is_train):\n",
        "    return {\n",
        "        input_brands: b_features[0],\n",
        "        input_makers: b_features[3],\n",
        "        input_products: b_features[4],\n",
        "        input_product_counts: b_features[5],\n",
        "        input_images: b_images,\n",
        "        labels: b_label,\n",
        "        is_train: b_is_train\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evYsER5OA8Rd",
        "colab_type": "text"
      },
      "source": [
        "### Training session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meSSK_xsA8Rd",
        "colab_type": "code",
        "outputId": "657f8b3f-121a-4849-bead-80203330d542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    tic = time()\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    writer = tf.summary.FileWriter(\n",
        "        base_path + '/logs/{}'.format(\n",
        "            datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        ),\n",
        "        sess.graph\n",
        "    )\n",
        "    \n",
        "    iteration = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\n",
        "            \"\\n\\nEpoch {0:03d} / {1:03d}\\n\"\n",
        "            .format(epoch, n_epochs)\n",
        "        )\n",
        "\n",
        "        training_loss = []\n",
        "        batch_iteration = 0\n",
        "        for b_features, b_images, b_label in tqdm(\n",
        "            generator(mode = 'training'),\n",
        "            total = math.ceil(len(train_labels) / batch_size)\n",
        "        ):\n",
        "            feed_dict = get_feed_dict(b_features, b_images, b_label, True)\n",
        "            \n",
        "            _, train_loss = sess.run(\n",
        "                [train_op, loss],\n",
        "                feed_dict=feed_dict\n",
        "            )\n",
        "\n",
        "            training_loss.append(train_loss)\n",
        "\n",
        "            iteration += 1\n",
        "            batch_iteration += 1\n",
        "            if batch_iteration % valid_freq != 0:\n",
        "                continue\n",
        "\n",
        "            # Write validation accuracy\n",
        "            top_1_i, top_5_i, loss_i = [], [], []\n",
        "            for b_features, b_images, b_label in generator('valid', 0.1):\n",
        "                feed_dict = get_feed_dict(b_features, b_images, b_label, False)\n",
        "                t1_acc_ij, t5_acc_ij, loss_ij = sess.run(\n",
        "                    [top1_acc, top5_acc, loss],\n",
        "                    feed_dict = feed_dict\n",
        "                )\n",
        "\n",
        "                top_1_i.append(t1_acc_ij)\n",
        "                top_5_i.append(t5_acc_ij)\n",
        "                loss_i.append(loss_ij)\n",
        "            \n",
        "            summary = tf.Summary()\n",
        "            summary.value.add(tag=\"val_t1_acc\", simple_value=np.mean(top_1_i))\n",
        "            summary.value.add(tag=\"val_t5_acc\", simple_value=np.mean(top_5_i))\n",
        "            summary.value.add(tag=\"train_loss\", simple_value=np.mean(\n",
        "                training_loss[batch_iteration - valid_freq:]\n",
        "            ))\n",
        "            summary.value.add(tag=\"val_loss\", simple_value=np.mean(loss_i))\n",
        "            \n",
        "            writer.add_summary(summary, iteration)\n",
        "            writer.flush()\n",
        "\n",
        "        toc = time()\n",
        "        print(\n",
        "            \"[*] TRAIN Loss {0:.4f} | Time {1:.2f}s\"\n",
        "            .format(np.mean(training_loss), toc - tic)\n",
        "        )\n",
        "        \n",
        "        if (epoch + 1) % valid_freq_epoch == 0:\n",
        "            top_1, top_5 = [], []\n",
        "\n",
        "            for b_features, b_images, b_label in generator('valid'):\n",
        "                feed_dict = get_feed_dict(b_features, b_images, b_label, False)\n",
        "                \n",
        "                t1_acc, t5_acc = sess.run(\n",
        "                    [top1_acc, top5_acc],\n",
        "                    feed_dict=feed_dict\n",
        "                )\n",
        "\n",
        "                top_1.append(t1_acc)\n",
        "                top_5.append(t5_acc)\n",
        "\n",
        "            print(\n",
        "                \"[*] VALIDATION Top-1 Acc: {0:.4f} | Top-5 Acc: {1:.4f}\"\n",
        "                .format(np.mean(top_1), np.mean(top_5))\n",
        "            )\n",
        "\n",
        "    saver.save(sess, base_path + '/models/models')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1563 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch 000 / 003\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1562/1563 [01:51<00:00, 13.95it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[*] TRAIN Loss 2.4103 | Time 116.17s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 2/1563 [00:00<01:55, 13.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[*] VALIDATION Top-1 Acc: 0.2687 | Top-5 Acc: 0.5739\n",
            "\n",
            "\n",
            "Epoch 001 / 003\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1562/1563 [01:49<00:00, 15.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[*] TRAIN Loss 0.8254 | Time 232.27s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 2/1563 [00:00<01:53, 13.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[*] VALIDATION Top-1 Acc: 0.3196 | Top-5 Acc: 0.6294\n",
            "\n",
            "\n",
            "Epoch 002 / 003\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|▌         | 94/1563 [00:05<01:34, 15.57it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-348d1d926251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             _, train_loss = sess.run(\n\u001b[1;32m     28\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwH2PQ8U1Ezg",
        "colab_type": "text"
      },
      "source": [
        "### Show Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIxAFcmh1HIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"./drive/My Drive/CoE202TermProject/logs\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHpFrEqj4r0L",
        "colab_type": "text"
      },
      "source": [
        "## Test process\n",
        "### Load the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwR2OFUesK7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test data\n",
        "with gzip.open('./drive/My Drive/CoE202TermProject/datasets/test.chunk.pickle', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "with gzip.open('./drive/My Drive/CoE202TermProject/datasets/valid.image.feats.pickle', 'rb') as f:\n",
        "    test_images = pickle.load(f)\n",
        "\n",
        "test_features  = test_data['features'][:]\n",
        "pids = test_data['pids'][:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b62j6uqe46sJ",
        "colab_type": "text"
      },
      "source": [
        "### Batch generator for test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp1YnA1-5AA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    saver = tf.train.import_meta_graph('./drive/My Drive/CoE202TermProject/models/models.meta')\n",
        "    saver.restore(sess, tf.train.latest_checkpoint('./drive/My Drive/CoE202TermProject/models/'))\n",
        "    DNN = tf.get_default_graph()\n",
        "\n",
        "    preds = []\n",
        "    for b_features, b_images in generator(mode='test'):\n",
        "        feed_dict = {\n",
        "            DNN.get_tensor_by_name('input_brands:0'): b_features[0],\n",
        "            DNN.get_tensor_by_name('input_makers:0'): b_features[3],\n",
        "            DNN.get_tensor_by_name('input_products:0'): b_features[4],\n",
        "            DNN.get_tensor_by_name('input_product_counts:0'): b_features[5],\n",
        "            DNN.get_tensor_by_name('input_prices:0'): b_features[6],\n",
        "            \n",
        "            DNN.get_tensor_by_name('input_images:0'): b_images,\n",
        "            DNN.get_tensor_by_name('is_train:0'): False\n",
        "        }\n",
        "\n",
        "        pred = sess.run(DNN.get_tensor_by_name('predictions:0'), feed_dict=feed_dict)\n",
        "        preds.extend(pred)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlrExwdK5CEq",
        "colab_type": "text"
      },
      "source": [
        "### Save the submission files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OXH2IxT5FuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Indexing of predictions\n",
        "argpreds = np.argmax(preds, axis=1)\n",
        "\n",
        "# Load label dictionary\n",
        "with open('./drive/My Drive/CoE202TermProject/datasets/y_vocab.pickle', 'rb') as f:\n",
        "    y_dict = pickle.load(f)\n",
        "# y_dict = pickle.loads(open('./drive/My Drive/CoE202TermProject/datasets/y_vocab.pickle').read())\n",
        "\n",
        "# Inverse label dictionary\n",
        "inv_y_dict = dict((y,x) for x,y in y_dict.items())\n",
        "submissions = [inv_y_dict[argpred] for argpred in argpreds]\n",
        "\n",
        "# Write the results to 'submissions.csv'\n",
        "f = open('./drive/My Drive/CoE202TermProject/submissions.csv', 'w')\n",
        "for i, j in zip(pids, submissions):\n",
        "    line = '{},{}\\n'.format(i,j)\n",
        "    f.write(line)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwd53pai5i3i",
        "colab_type": "text"
      },
      "source": [
        "You should submit the 'submissions.csv' file and 4 tf.save files ('checkpoint', 'dnn_models.data', 'dnn_models.index', 'dnn_models.meta') in models folder"
      ]
    }
  ]
}