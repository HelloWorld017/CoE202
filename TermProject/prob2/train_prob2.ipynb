{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"train_prob2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"V1Xr_8TXA8Q9","colab_type":"code","outputId":"5679f4c3-5aba-4ee8-abef-0743500a2e24","executionInfo":{"status":"ok","timestamp":1576768164966,"user_tz":-540,"elapsed":3795,"user":{"displayName":"Khi nenw","photoUrl":"","userId":"08455783256192856833"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","import keras\n","import pickle\n","import h5py\n","import time\n","import gzip\n","\n","from tensorflow.python.ops import data_flow_ops\n","from tensorflow.keras.layers import Embedding"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Cg4QjiBNBMgH","colab_type":"code","outputId":"37511335-3895-41b3-dad0-82f50edc7b47","executionInfo":{"status":"ok","timestamp":1576768164967,"user_tz":-540,"elapsed":3782,"user":{"displayName":"Khi nenw","photoUrl":"","userId":"08455783256192856833"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lgg_pXshA8RF","colab_type":"text"},"source":["### Load the input vectors"]},{"cell_type":"code","metadata":{"id":"NOXi9akbwHZB","colab_type":"code","colab":{}},"source":["with gzip.open('./drive/My Drive/CoE202TermProject/datasets/train.chunk.pickle', 'rb') as f:\n","    traindata = pickle.load(f)\n","\n","with gzip.open('./drive/My Drive/CoE202TermProject/datasets/train.image.feats.pickle', 'rb') as f:\n","    train_images = pickle.load(f)\n","\n","train_product = traindata['product'][:]\n","train_label = traindata['label'][:]\n","\n","\n","with gzip.open('./drive/My Drive/CoE202TermProject/datasets/valid.chunk.pickle', 'rb') as f:\n","    validdata = pickle.load(f)\n","\n","with gzip.open('./drive/My Drive/CoE202TermProject/datasets/valid.image.feats.pickle', 'rb') as f:\n","    valid_images = pickle.load(f)\n","\n","valid_product = validdata['product'][:]\n","valid_label = validdata['label'][:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vlaW3FIsA8RK","colab_type":"code","colab":{}},"source":["batch_size = 512\n","lr         = 1e-4\n","n_epochs   = 3\n","valid_freq = 3\n","n_classes  = np.max(train_label) + 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XtxIYrsYA8RP","colab_type":"text"},"source":["### Placeholders"]},{"cell_type":"code","metadata":{"id":"6txbjX2tA8RR","colab_type":"code","colab":{}},"source":["input_images = tf.placeholder(dtype=tf.float32, shape=(None, 2048), name='input_images')\n","input_prices = tf.placeholder(dtype=tf.float32, shape=(None, ), name='input_prices')\n","labels = tf.placeholder(dtype=tf.int32, shape=(None, ), name='labels')\n","is_train = tf.placeholder(dtype=tf.bool, name='is_train')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZw_gtkeA8RT","colab_type":"text"},"source":["### Deep neural network"]},{"cell_type":"code","metadata":{"id":"5o6qR3E3A8RU","colab_type":"code","outputId":"c1a13d1b-a126-4b8f-a396-b67863d14434","executionInfo":{"status":"ok","timestamp":1576768224986,"user_tz":-540,"elapsed":63768,"user":{"displayName":"Khi nenw","photoUrl":"","userId":"08455783256192856833"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["# Processing Images\n","x_images = tf.keras.layers.Dense(n_classes * 2)(input_images)\n","x_images = tf.layers.batch_normalization(x_images, training=is_train)\n","x_images = tf.nn.relu(x_images)\n","\n","hidden1 = tf.keras.layers.Dense(n_classes)(x_images)\n","hidden1 = tf.layers.batch_normalization(hidden1, training=is_train)\n","hidden1 = tf.nn.relu(hidden1)\n","\n","# Normalizing Prices\n","x_prices = tf.reshape(input_prices, (-1, 1))\n","x_prices = tf.layers.batch_normalization(x_prices, training=is_train)\n","\n","# Concatenating Images and Prices\n","x_concat = tf.keras.layers.Concatenate(axis=1)([hidden1, x_prices])\n","\n","# Hidden Layer\n","hidden2 = tf.keras.layers.Dense(n_classes)(x_concat)\n","hidden2 = tf.layers.batch_normalization(hidden2, training=is_train)\n","hidden2 = tf.nn.relu(hidden2)\n","\n","# Output\n","logits = tf.keras.layers.Dense(n_classes)(hidden2)\n","\n","# Softmax\n","preds = tf.nn.softmax(logits, name='predictions')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From <ipython-input-6-bb2206535878>:2: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dye2seUzA8RX","colab_type":"text"},"source":["### Loss funciton & Optimizer"]},{"cell_type":"code","metadata":{"id":"SSTWGvtlA8RZ","colab_type":"code","outputId":"48d3c73b-e378-4cdf-a52c-93499ca193f8","executionInfo":{"status":"ok","timestamp":1576768225930,"user_tz":-540,"elapsed":64695,"user":{"displayName":"Khi nenw","photoUrl":"","userId":"08455783256192856833"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["# Softmax cross entropy loss\n","loss = tf.losses.softmax_cross_entropy(onehot_labels=tf.one_hot(labels, n_classes), logits=logits)\n","\n","# Weight decay\n","reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n","loss = tf.add_n([loss] + reg_losses, name='total_loss')\n","\n","# Optimizer\n","optm = tf.train.AdamOptimizer(lr)\n","train_op = optm.minimize(loss, global_step=tf.train.get_global_step(), name='step_update')\n","update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","train_op = tf.group([train_op, update_ops])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_H7eVZH7G_ij","colab_type":"text"},"source":["### Accuracy"]},{"cell_type":"code","metadata":{"id":"B9PfnmlmG_5w","colab_type":"code","colab":{}},"source":["top1_acc = tf.keras.metrics.top_k_categorical_accuracy(y_true=tf.one_hot(labels, n_classes),\n","                                                        y_pred=preds, k=1)\n","top1_acc = tf.identity(top1_acc, name='top1_acc')\n","\n","top5_acc = tf.keras.metrics.top_k_categorical_accuracy(y_true=tf.one_hot(labels, n_classes),\n","                                                        y_pred=preds, k=5)\n","top5_acc = tf.identity(top5_acc, name='top5_acc')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jkUB1YJpA8Rb","colab_type":"text"},"source":["### Batch generator"]},{"cell_type":"code","metadata":{"id":"-dypzaB9A8Rb","colab_type":"code","colab":{}},"source":["def generator(mode='training'):\n","    if mode == 'training':\n","        products = train_product\n","        labels = train_label\n","        images = train_images\n","\n","    elif mode == 'valid':\n","        products = valid_product\n","        labels = valid_label\n","        images = valid_images\n","    \n","    elif mode == 'test':\n","        products = test_product\n","        labels = None\n","        images = test_images\n","\n","    n_data = len(products)\n","    indices = np.arange(n_data)\n","    np.random.shuffle(indices)\n","    \n","    for start_idx in range(0, n_data, batch_size):\n","        if start_idx + batch_size <= n_data:\n","            excerpt = indices[start_idx: start_idx + batch_size]\n","\n","            if labels is not None:\n","                yield list(zip(*products[excerpt, :])), \\\n","                    images[excerpt, :], \\\n","                    labels[excerpt]\n","            \n","            else:\n","                yield list(zip(*products[excerpt, :])), \\\n","                    images[excerpt, :]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"evYsER5OA8Rd","colab_type":"text"},"source":["### Training session"]},{"cell_type":"code","metadata":{"id":"meSSK_xsA8Rd","colab_type":"code","outputId":"5f03190d-0b43-40ab-b104-2ceeb1fc73ce","executionInfo":{"status":"ok","timestamp":1576768562606,"user_tz":-540,"elapsed":401347,"user":{"displayName":"Khi nenw","photoUrl":"","userId":"08455783256192856833"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["import tqdm\n","import math\n","\n","with tf.Session() as sess:\n","    tic = time.time()\n","    saver = tf.train.Saver()\n","    sess.run(tf.global_variables_initializer())\n","    \n","    for epoch in range(n_epochs):\n","        print(\"\\n\\nEpoch {0:03d} / {1:03d}\\n\".format(epoch, n_epochs))\n","        training_loss = []\n","        for b_product, b_images, b_label in tqdm.tqdm(\n","            generator(mode='training'),\n","            total=math.ceil(len(train_label) / batch_size)\n","        ):\n","            feed_dict = {\n","                input_prices: b_product[6],\n","                input_images: b_images,\n","                labels: b_label,\n","                is_train: True\n","            }\n","            \n","            _, train_loss = sess.run(\n","                [train_op, loss],\n","                feed_dict=feed_dict\n","            )\n","\n","            training_loss.append(train_loss)\n","\n","        toc = time.time()\n","        print(\n","            \"[*] TRAIN Loss {0:.4f} | Time {1:.2f}s\"\n","            .format(np.mean(training_loss), toc - tic)\n","        )\n","        \n","        if (epoch+1) % valid_freq == 0:\n","            top_1, top_5 = [], []\n","            for b_product, b_images, b_label in generator(mode='valid'):\n","                feed_dict = {\n","                    input_prices: b_product[6],\n","                    input_images: b_images,\n","                    labels: b_label,\n","                    is_train: True\n","                }\n","                    \n","                t1_acc, t5_acc = sess.run(\n","                    [top1_acc, top5_acc],\n","                    feed_dict=feed_dict\n","                )\n","\n","                top_1.append(t1_acc)\n","                top_5.append(t5_acc)\n","\n","            print(\n","                \"[*] VALIDATION Top-1 Acc: {0:.4f} | Top-5 Acc: {1:.4f}\"\n","                .format(np.mean(top_1), np.mean(top_5))\n","            )\n","\n","    saver.save(sess, './drive/My Drive/CoE202TermProject/models/models')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/1563 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","Epoch 000 / 003\n","\n"],"name":"stdout"},{"output_type":"stream","text":["100%|█████████▉| 1562/1563 [01:47<00:00, 14.56it/s]\n","  0%|          | 2/1563 [00:00<02:01, 12.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["[*] TRAIN Loss 2.1437 | Time 110.52s\n","\n","\n","Epoch 001 / 003\n","\n"],"name":"stdout"},{"output_type":"stream","text":["100%|█████████▉| 1562/1563 [01:46<00:00, 14.86it/s]\n","  0%|          | 2/1563 [00:00<01:57, 13.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["[*] TRAIN Loss 1.1751 | Time 217.06s\n","\n","\n","Epoch 002 / 003\n","\n"],"name":"stdout"},{"output_type":"stream","text":["100%|█████████▉| 1562/1563 [01:46<00:00, 14.80it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["[*] TRAIN Loss 0.8341 | Time 323.25s\n","[*] VALIDATION Top-1 Acc: 0.6886 | Top-5 Acc: 0.9003\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YHpFrEqj4r0L","colab_type":"text"},"source":["## Test process\n","### Load the test data"]},{"cell_type":"code","metadata":{"id":"KwR2OFUesK7C","colab_type":"code","colab":{}},"source":["# test data\n","with gzip.open('./drive/My Drive/CoE202TermProject/datasets/test.chunk.pickle', 'rb') as f:\n","    testdata = pickle.load(f)\n","\n","with gzip.open('./drive/My Drive/CoE202TermProject/datasets/valid.image.feats.pickle', 'rb') as f:\n","    test_images = pickle.load(f)\n","\n","test_product  = testdata['product'][:]\n","pids          = testdata['pids'][:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b62j6uqe46sJ","colab_type":"text"},"source":["### Batch generator for test dataset"]},{"cell_type":"code","metadata":{"id":"pp1YnA1-5AA1","colab_type":"code","outputId":"9df1f9f1-c2b4-416e-ba4c-caaf684dc99c","executionInfo":{"status":"ok","timestamp":1576768584858,"user_tz":-540,"elapsed":423582,"user":{"displayName":"Khi nenw","photoUrl":"","userId":"08455783256192856833"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["with tf.Session() as sess:\n","    saver = tf.train.import_meta_graph('./drive/My Drive/CoE202TermProject/models/models.meta')\n","    saver.restore(sess, tf.train.latest_checkpoint('./drive/My Drive/CoE202TermProject/models/'))\n","    DNN = tf.get_default_graph()\n","\n","    preds = []\n","    for b_product, b_images in generator(mode='test'):\n","        feed_dict = {\n","            DNN.get_tensor_by_name('input_images:0'): b_images,\n","            DNN.get_tensor_by_name('input_prices:0'): b_product[6],\n","            DNN.get_tensor_by_name('is_train:0'): False\n","        }\n","\n","        pred = sess.run(DNN.get_tensor_by_name('predictions:0'), feed_dict=feed_dict)\n","        preds.extend(pred)\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./drive/My Drive/CoE202TermProject/models/models\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IlrExwdK5CEq","colab_type":"text"},"source":["### Save the submission files"]},{"cell_type":"code","metadata":{"id":"4OXH2IxT5FuX","colab_type":"code","colab":{}},"source":["# Indexing of predictions\n","argpreds = np.argmax(preds, axis=1)\n","\n","# Load label dictionary\n","with open('./drive/My Drive/CoE202TermProject/datasets/y_vocab.pickle', 'rb') as f:\n","    y_dict = pickle.load(f)\n","# y_dict = pickle.loads(open('./drive/My Drive/CoE202TermProject/datasets/y_vocab.pickle').read())\n","\n","# Inverse label dictionary\n","inv_y_dict = dict((y,x) for x,y in y_dict.items())\n","submissions = [inv_y_dict[argpred] for argpred in argpreds]\n","\n","# Write the results to 'submissions.csv'\n","f = open('./drive/My Drive/CoE202TermProject/submissions.csv', 'w')\n","for i, j in zip(pids, submissions):\n","    line = '{},{}\\n'.format(i,j)\n","    f.write(line)\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vwd53pai5i3i","colab_type":"text"},"source":["You should submit the 'submissions.csv' file and 4 tf.save files ('checkpoint', 'dnn_models.data', 'dnn_models.index', 'dnn_models.meta') in models folder"]}]}