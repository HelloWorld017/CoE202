{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "V1Xr_8TXA8Q9",
    "outputId": "be03c1cd-9345-4246-88ee-2f9e53203134"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import pickle\n",
    "import h5py\n",
    "import time\n",
    "import gzip\n",
    "\n",
    "from tensorflow.python.ops import data_flow_ops\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lgg_pXshA8RF"
   },
   "source": [
    "### Load the input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NOXi9akbwHZB"
   },
   "outputs": [],
   "source": [
    "with gzip.open('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/train.chunk.pickle', 'rb') as f:\n",
    "    traindata = pickle.load(f)\n",
    "\n",
    "train_product   = traindata['product'][:]\n",
    "train_wproduct = traindata['w_product'][:]\n",
    "train_label   = traindata['label'][:]\n",
    "\n",
    "with gzip.open('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/valid.chunk.pickle', 'rb') as f:\n",
    "    validdata = pickle.load(f)\n",
    "\n",
    "valid_product   = validdata['product'][:]\n",
    "valid_wproduct  = validdata['w_product'][:]\n",
    "valid_label   = validdata['label'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlaW3FIsA8RK"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "embd_size  = 1024\n",
    "voca_size  = 100001\n",
    "lr         = 1e-4\n",
    "n_epochs   = 3\n",
    "valid_freq = 3\n",
    "max_len    = np.shape(train_product)[1]\n",
    "n_classes  = np.max(train_label) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtxIYrsYA8RP"
   },
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6txbjX2tA8RR"
   },
   "outputs": [],
   "source": [
    "products   = tf.placeholder(dtype=tf.float32, shape=[None, max_len], name='products')\n",
    "w_products = tf.placeholder(dtype=tf.float32, shape=[None, max_len], name='weight_products')\n",
    "labels   = tf.placeholder(dtype=tf.int32, shape=[None], name='labels')\n",
    "is_train = tf.placeholder(dtype=tf.bool, name='is_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZw_gtkeA8RT"
   },
   "source": [
    "### Deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "5o6qR3E3A8RU",
    "outputId": "74980aed-ed74-4493-cc18-862bd96dd3bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-6-512459554307>:12: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-512459554307>:13: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From <ipython-input-6-512459554307>:17: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    }
   ],
   "source": [
    "# Define embedding\n",
    "embd = Embedding(voca_size, embd_size, name='product_embd')                     # Define embedding function\n",
    "\n",
    "# Product embedding\n",
    "t_product_embd = embd(products)                                                 # Get the embedding vector of products\n",
    "\n",
    "# Multiply the weights of product vectors\n",
    "w_product_mat = tf.reshape(w_products, [-1, max_len, 1])\n",
    "product_embd_mat = tf.keras.layers.dot([t_product_embd, w_product_mat], axes=1) # Multiply the counts of products to the corresponding embedding vectors\n",
    "product_embd = tf.reshape(product_embd_mat, [-1, embd_size])\n",
    "\n",
    "# Non-linear activations\n",
    "embd_out = tf.layers.dropout(product_embd, training=is_train)                   # Dropout\n",
    "bn = tf.layers.batch_normalization(embd_out, training=is_train)                 # Batch normalization\n",
    "relu = tf.nn.relu(bn)                                                           # ReLU function\n",
    "\n",
    "# logits\n",
    "logits = tf.layers.dense(relu, n_classes)                                       # Fully-connected layer\n",
    "\n",
    "# Softmax\n",
    "preds = tf.nn.softmax(logits, name='predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dye2seUzA8RX"
   },
   "source": [
    "### Loss funciton & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "SSTWGvtlA8RZ",
    "outputId": "5110ac64-8447-4878-c62b-1ec4861b7e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Softmax cross entropy loss\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=tf.one_hot(labels, n_classes), logits=logits)\n",
    "\n",
    "# Weight decay\n",
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([loss] + reg_losses, name='total_loss')\n",
    "\n",
    "# Optimizer\n",
    "optm = tf.train.AdamOptimizer(lr)\n",
    "train_op = optm.minimize(loss, global_step=tf.train.get_global_step(), name='step_update')\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "train_op = tf.group([train_op, update_ops])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_H7eVZH7G_ij"
   },
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9PfnmlmG_5w"
   },
   "outputs": [],
   "source": [
    "top1_acc = tf.keras.metrics.top_k_categorical_accuracy(y_true=tf.one_hot(labels, n_classes),\n",
    "                                                        y_pred=preds, k=1)\n",
    "top1_acc = tf.identity(top1_acc, name='top1_acc')\n",
    "\n",
    "top5_acc = tf.keras.metrics.top_k_categorical_accuracy(y_true=tf.one_hot(labels, n_classes),\n",
    "                                                        y_pred=preds, k=5)\n",
    "top5_acc = tf.identity(top5_acc, name='top5_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkUB1YJpA8Rb"
   },
   "source": [
    "### Batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dypzaB9A8Rb"
   },
   "outputs": [],
   "source": [
    "def generator(mode='training'):\n",
    "    if mode == 'training':\n",
    "        n_data = len(train_label)\n",
    "        indices = np.arange(n_data)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for start_idx in range(0, n_data, batch_size):\n",
    "            if start_idx + batch_size <= n_data:\n",
    "                excerpt = indices[start_idx: start_idx + batch_size]\n",
    "                yield train_product[excerpt, :], train_wproduct[excerpt, :], train_label[excerpt]\n",
    "\n",
    "    elif mode == 'valid':\n",
    "        n_data = len(valid_label)\n",
    "        indices = np.arange(n_data)\n",
    "\n",
    "        for start_idx in range(0, n_data, batch_size):\n",
    "            if start_idx + batch_size <= n_data:\n",
    "                excerpt = indices[start_idx: start_idx + batch_size]\n",
    "                yield valid_product[excerpt, :], valid_wproduct[excerpt, :], valid_label[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evYsER5OA8Rd"
   },
   "source": [
    "### Training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "meSSK_xsA8Rd",
    "outputId": "82217e8e-4068-4ce8-f1c6-71ce689b4d21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 000 / 003\n",
      "\n",
      "[*] TRAIN Loss 4.5290 | Time 50.23s\n",
      "\n",
      "\n",
      "Epoch 001 / 003\n",
      "\n",
      "[*] TRAIN Loss 2.0962 | Time 97.14s\n",
      "\n",
      "\n",
      "Epoch 002 / 003\n",
      "\n",
      "[*] TRAIN Loss 1.2322 | Time 144.04s\n",
      "[*] VALIDATION Top-1 Acc: 0.7439 | Top-5 Acc: 0.9065\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tic = time.time()\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"\\n\\nEpoch {0:03d} / {1:03d}\\n\".format(epoch, n_epochs))\n",
    "        training_loss = []\n",
    "        for b_product, bw_product, b_label in generator(mode='training'):\n",
    "            feed_dict = {products  : b_product,\n",
    "                         w_products: bw_product,\n",
    "                         labels    : b_label,\n",
    "                         is_train  : True}\n",
    "            _, train_loss = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "            training_loss.append(train_loss)\n",
    "\n",
    "        toc = time.time()\n",
    "        print(\"[*] TRAIN Loss {0:.4f} | Time {1:.2f}s\".format(np.mean(training_loss), toc - tic))\n",
    "        \n",
    "        if (epoch+1) % valid_freq == 0:\n",
    "            TOP1, TOP5 = [], []\n",
    "            for b_product, bw_product, b_label in generator(mode='valid'):\n",
    "                feed_dict = {products : b_product,\n",
    "                            w_products: bw_product,\n",
    "                            labels    : b_label,\n",
    "                            is_train  : False}\n",
    "                t1_acc, t5_acc = sess.run([top1_acc, top5_acc], feed_dict=feed_dict)\n",
    "                TOP1.append(t1_acc)\n",
    "                TOP5.append(t5_acc)\n",
    "            print(\"[*] VALIDATION Top-1 Acc: {0:.4f} | Top-5 Acc: {1:.4f}\".format(np.mean(TOP1), np.mean(TOP5)))\n",
    "\n",
    "    saver.save(sess, './drive/My Drive/Colab Notebooks/CoE202_KakaoArena/models/dnn_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHpFrEqj4r0L"
   },
   "source": [
    "## Test process\n",
    "### Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KwR2OFUesK7C"
   },
   "outputs": [],
   "source": [
    "# test data\n",
    "with gzip.open('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/test.chunk.pickle', 'rb') as f:\n",
    "    testdata = pickle.load(f)\n",
    "\n",
    "test_product  = testdata['product'][:]\n",
    "test_wproduct = testdata['w_product'][:]\n",
    "pids          = testdata['pids'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b62j6uqe46sJ"
   },
   "source": [
    "### Batch generator for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYyChL_944V-"
   },
   "outputs": [],
   "source": [
    "def generator(mode='test'):\n",
    "    if mode == 'test':\n",
    "        n_data = len(test_product)\n",
    "        indices = np.arange(n_data)\n",
    "        \n",
    "        for start_idx in range(0, n_data, batch_size):\n",
    "            excerpt = indices[start_idx: start_idx + batch_size]\n",
    "            yield test_product[excerpt, :], test_wproduct[excerpt, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pp1YnA1-5AA1",
    "outputId": "b731b360-699b-474e-d97c-b5da612b17ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/models/dnn_models\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/models/dnn_models.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/models'))\n",
    "    DNN = tf.get_default_graph()\n",
    "\n",
    "    preds = []\n",
    "    for b_product, bw_product in generator(mode='test'):\n",
    "        feed_dict = {DNN.get_tensor_by_name('products:0')       : b_product,\n",
    "                     DNN.get_tensor_by_name('weight_products:0'): bw_product,\n",
    "                     DNN.get_tensor_by_name('is_train:0')     : False}\n",
    "\n",
    "        pred = sess.run(DNN.get_tensor_by_name('predictions:0'), feed_dict=feed_dict)\n",
    "        preds.extend(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlrExwdK5CEq"
   },
   "source": [
    "### Save the submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OXH2IxT5FuX"
   },
   "outputs": [],
   "source": [
    "# Indexing of predictions\n",
    "argpreds = np.argmax(preds, axis=1)\n",
    "\n",
    "# Load label dictionary\n",
    "with open('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/y_vocab.pickle', 'rb') as f:\n",
    "    y_dict = pickle.load(f)\n",
    "# y_dict = pickle.loads(open('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/y_vocab.pickle').read())\n",
    "\n",
    "# Inverse label dictionary\n",
    "inv_y_dict = dict((y,x) for x,y in y_dict.items())\n",
    "submissions = [inv_y_dict[argpred] for argpred in argpreds]\n",
    "\n",
    "# Write the results to 'submissions.csv'\n",
    "f = open('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/submissions.csv', 'w')\n",
    "for i, j in zip(pids, submissions):\n",
    "    line = '{},{}\\n'.format(i,j)\n",
    "    f.write(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vwd53pai5i3i"
   },
   "source": [
    "You should submit the 'submissions.csv' file and 4 tf.save files ('checkpoint', 'dnn_models.data', 'dnn_models.index', 'dnn_models.meta') in models folder"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
