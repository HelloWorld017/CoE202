{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1Xr_8TXA8Q9",
        "colab_type": "code",
        "outputId": "be03c1cd-9345-4246-88ee-2f9e53203134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "import pickle\n",
        "import h5py\n",
        "import time\n",
        "import gzip\n",
        "\n",
        "from tensorflow.python.ops import data_flow_ops\n",
        "from tensorflow.keras.layers import Embedding"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg4QjiBNBMgH",
        "colab_type": "code",
        "outputId": "5781fbf2-cbfe-41e2-ccd2-b2383cae0ae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgg_pXshA8RF",
        "colab_type": "text"
      },
      "source": [
        "### Load the input vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOXi9akbwHZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with gzip.open('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/train.chunk.pickle', 'rb') as f:\n",
        "    traindata = pickle.load(f)\n",
        "\n",
        "train_product   = traindata['product'][:]\n",
        "train_wproduct = traindata['w_product'][:]\n",
        "train_label   = traindata['label'][:]\n",
        "\n",
        "with gzip.open('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/valid.chunk.pickle', 'rb') as f:\n",
        "    validdata = pickle.load(f)\n",
        "\n",
        "valid_product   = validdata['product'][:]\n",
        "valid_wproduct  = validdata['w_product'][:]\n",
        "valid_label   = validdata['label'][:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlaW3FIsA8RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 512\n",
        "embd_size  = 1024\n",
        "voca_size  = 100001\n",
        "lr         = 1e-4\n",
        "n_epochs   = 3\n",
        "valid_freq = 3\n",
        "max_len    = np.shape(train_product)[1]\n",
        "n_classes  = np.max(train_label) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtxIYrsYA8RP",
        "colab_type": "text"
      },
      "source": [
        "### Placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6txbjX2tA8RR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "products   = tf.placeholder(dtype=tf.float32, shape=[None, max_len], name='products')\n",
        "w_products = tf.placeholder(dtype=tf.float32, shape=[None, max_len], name='weight_products')\n",
        "labels   = tf.placeholder(dtype=tf.int32, shape=[None], name='labels')\n",
        "is_train = tf.placeholder(dtype=tf.bool, name='is_train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZw_gtkeA8RT",
        "colab_type": "text"
      },
      "source": [
        "### Deep neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o6qR3E3A8RU",
        "colab_type": "code",
        "outputId": "74980aed-ed74-4493-cc18-862bd96dd3bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# Define embedding\n",
        "embd = Embedding(voca_size, embd_size, name='product_embd')                     # Define embedding function\n",
        "\n",
        "# Product embedding\n",
        "t_product_embd = embd(products)                                                 # Get the embedding vector of products\n",
        "\n",
        "# Multiply the weights of product vectors\n",
        "w_product_mat = tf.reshape(w_products, [-1, max_len, 1])\n",
        "product_embd_mat = tf.keras.layers.dot([t_product_embd, w_product_mat], axes=1) # Multiply the counts of products to the corresponding embedding vectors\n",
        "product_embd = tf.reshape(product_embd_mat, [-1, embd_size])\n",
        "\n",
        "# Non-linear activations\n",
        "embd_out = tf.layers.dropout(product_embd, training=is_train)                   # Dropout\n",
        "bn = tf.layers.batch_normalization(embd_out, training=is_train)                 # Batch normalization\n",
        "relu = tf.nn.relu(bn)                                                           # ReLU function\n",
        "\n",
        "# logits\n",
        "logits = tf.layers.dense(relu, n_classes)                                       # Fully-connected layer\n",
        "\n",
        "# Softmax\n",
        "preds = tf.nn.softmax(logits, name='predictions')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From <ipython-input-6-512459554307>:12: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-512459554307>:13: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From <ipython-input-6-512459554307>:17: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dye2seUzA8RX",
        "colab_type": "text"
      },
      "source": [
        "### Loss funciton & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSTWGvtlA8RZ",
        "colab_type": "code",
        "outputId": "5110ac64-8447-4878-c62b-1ec4861b7e50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Softmax cross entropy loss\n",
        "loss = tf.losses.softmax_cross_entropy(onehot_labels=tf.one_hot(labels, n_classes), logits=logits)\n",
        "\n",
        "# Weight decay\n",
        "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "loss = tf.add_n([loss] + reg_losses, name='total_loss')\n",
        "\n",
        "# Optimizer\n",
        "optm = tf.train.AdamOptimizer(lr)\n",
        "train_op = optm.minimize(loss, global_step=tf.train.get_global_step(), name='step_update')\n",
        "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "train_op = tf.group([train_op, update_ops])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H7eVZH7G_ij",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9PfnmlmG_5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top1_acc = tf.keras.metrics.top_k_categorical_accuracy(y_true=tf.one_hot(labels, n_classes),\n",
        "                                                        y_pred=preds, k=1)\n",
        "top1_acc = tf.identity(top1_acc, name='top1_acc')\n",
        "\n",
        "top5_acc = tf.keras.metrics.top_k_categorical_accuracy(y_true=tf.one_hot(labels, n_classes),\n",
        "                                                        y_pred=preds, k=5)\n",
        "top5_acc = tf.identity(top5_acc, name='top5_acc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkUB1YJpA8Rb",
        "colab_type": "text"
      },
      "source": [
        "### Batch generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dypzaB9A8Rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(mode='training'):\n",
        "    if mode == 'training':\n",
        "        n_data = len(train_label)\n",
        "        indices = np.arange(n_data)\n",
        "        np.random.shuffle(indices)\n",
        "        \n",
        "        for start_idx in range(0, n_data, batch_size):\n",
        "            if start_idx + batch_size <= n_data:\n",
        "                excerpt = indices[start_idx: start_idx + batch_size]\n",
        "                yield train_product[excerpt, :], train_wproduct[excerpt, :], train_label[excerpt]\n",
        "\n",
        "    elif mode == 'valid':\n",
        "        n_data = len(valid_label)\n",
        "        indices = np.arange(n_data)\n",
        "\n",
        "        for start_idx in range(0, n_data, batch_size):\n",
        "            if start_idx + batch_size <= n_data:\n",
        "                excerpt = indices[start_idx: start_idx + batch_size]\n",
        "                yield valid_product[excerpt, :], valid_wproduct[excerpt, :], valid_label[excerpt]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evYsER5OA8Rd",
        "colab_type": "text"
      },
      "source": [
        "### Training session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meSSK_xsA8Rd",
        "colab_type": "code",
        "outputId": "82217e8e-4068-4ce8-f1c6-71ce689b4d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    tic = time.time()\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"\\n\\nEpoch {0:03d} / {1:03d}\\n\".format(epoch, n_epochs))\n",
        "        training_loss = []\n",
        "        for b_product, bw_product, b_label in generator(mode='training'):\n",
        "            feed_dict = {products  : b_product,\n",
        "                         w_products: bw_product,\n",
        "                         labels    : b_label,\n",
        "                         is_train  : True}\n",
        "            _, train_loss = sess.run([train_op, loss], feed_dict=feed_dict)\n",
        "            training_loss.append(train_loss)\n",
        "\n",
        "        toc = time.time()\n",
        "        print(\"[*] TRAIN Loss {0:.4f} | Time {1:.2f}s\".format(np.mean(training_loss), toc - tic))\n",
        "        \n",
        "        if (epoch+1) % valid_freq == 0:\n",
        "            TOP1, TOP5 = [], []\n",
        "            for b_product, bw_product, b_label in generator(mode='valid'):\n",
        "                feed_dict = {products : b_product,\n",
        "                            w_products: bw_product,\n",
        "                            labels    : b_label,\n",
        "                            is_train  : False}\n",
        "                t1_acc, t5_acc = sess.run([top1_acc, top5_acc], feed_dict=feed_dict)\n",
        "                TOP1.append(t1_acc)\n",
        "                TOP5.append(t5_acc)\n",
        "            print(\"[*] VALIDATION Top-1 Acc: {0:.4f} | Top-5 Acc: {1:.4f}\".format(np.mean(TOP1), np.mean(TOP5)))\n",
        "\n",
        "    saver.save(sess, './drive/My Drive/Colab Notebooks/CoE202_KakaoArena/models/dnn_models')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch 000 / 003\n",
            "\n",
            "[*] TRAIN Loss 4.5290 | Time 50.23s\n",
            "\n",
            "\n",
            "Epoch 001 / 003\n",
            "\n",
            "[*] TRAIN Loss 2.0962 | Time 97.14s\n",
            "\n",
            "\n",
            "Epoch 002 / 003\n",
            "\n",
            "[*] TRAIN Loss 1.2322 | Time 144.04s\n",
            "[*] VALIDATION Top-1 Acc: 0.7439 | Top-5 Acc: 0.9065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHpFrEqj4r0L",
        "colab_type": "text"
      },
      "source": [
        "## Test process\n",
        "### Load the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwR2OFUesK7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test data\n",
        "with gzip.open('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/test.chunk.pickle', 'rb') as f:\n",
        "    testdata = pickle.load(f)\n",
        "\n",
        "test_product  = testdata['product'][:]\n",
        "test_wproduct = testdata['w_product'][:]\n",
        "pids          = testdata['pids'][:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b62j6uqe46sJ",
        "colab_type": "text"
      },
      "source": [
        "### Batch generator for test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYyChL_944V-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(mode='test'):\n",
        "    if mode == 'test':\n",
        "        n_data = len(test_product)\n",
        "        indices = np.arange(n_data)\n",
        "        \n",
        "        for start_idx in range(0, n_data, batch_size):\n",
        "            excerpt = indices[start_idx: start_idx + batch_size]\n",
        "            yield test_product[excerpt, :], test_wproduct[excerpt, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp1YnA1-5AA1",
        "colab_type": "code",
        "outputId": "b731b360-699b-474e-d97c-b5da612b17ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    saver = tf.train.import_meta_graph('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/models/dnn_models.meta')\n",
        "    saver.restore(sess, tf.train.latest_checkpoint('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/models'))\n",
        "    DNN = tf.get_default_graph()\n",
        "\n",
        "    preds = []\n",
        "    for b_product, bw_product in generator(mode='test'):\n",
        "        feed_dict = {DNN.get_tensor_by_name('products:0')       : b_product,\n",
        "                     DNN.get_tensor_by_name('weight_products:0'): bw_product,\n",
        "                     DNN.get_tensor_by_name('is_train:0')     : False}\n",
        "\n",
        "        pred = sess.run(DNN.get_tensor_by_name('predictions:0'), feed_dict=feed_dict)\n",
        "        preds.extend(pred)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/models/dnn_models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlrExwdK5CEq",
        "colab_type": "text"
      },
      "source": [
        "### Save the submission files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OXH2IxT5FuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Indexing of predictions\n",
        "argpreds = np.argmax(preds, axis=1)\n",
        "\n",
        "# Load label dictionary\n",
        "with open('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/y_vocab.pickle', 'rb') as f:\n",
        "    y_dict = pickle.load(f)\n",
        "# y_dict = pickle.loads(open('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/y_vocab.pickle').read())\n",
        "\n",
        "# Inverse label dictionary\n",
        "inv_y_dict = dict((y,x) for x,y in y_dict.items())\n",
        "submissions = [inv_y_dict[argpred] for argpred in argpreds]\n",
        "\n",
        "# Write the results to 'submissions.csv'\n",
        "f = open('./drive/My Drive/Colab Notebooks/CoE202_KakaoArena/submissions.csv', 'w')\n",
        "for i, j in zip(pids, submissions):\n",
        "    line = '{},{}\\n'.format(i,j)\n",
        "    f.write(line)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwd53pai5i3i",
        "colab_type": "text"
      },
      "source": [
        "You should submit the 'submissions.csv' file and 4 tf.save files ('checkpoint', 'dnn_models.data', 'dnn_models.index', 'dnn_models.meta') in models folder"
      ]
    }
  ]
}