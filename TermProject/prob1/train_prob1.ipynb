{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"train_prob1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"V1Xr_8TXA8Q9","colab_type":"code","outputId":"fa1b14f3-474d-4a6c-9fb9-dcc37d4f5ed6","executionInfo":{"status":"ok","timestamp":1576757316049,"user_tz":-540,"elapsed":4368,"user":{"displayName":"Khi nenw","photoUrl":"","userId":"08455783256192856833"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","import keras\n","import pickle\n","import h5py\n","import time\n","import gzip\n","\n","from tensorflow.python.ops import data_flow_ops\n","from tensorflow.keras.layers import Embedding"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Cg4QjiBNBMgH","colab_type":"code","outputId":"4881c7d3-8f0e-4a6e-d7d2-a287af959060","executionInfo":{"status":"ok","timestamp":1576757316051,"user_tz":-540,"elapsed":4351,"user":{"displayName":"Khi nenw","photoUrl":"","userId":"08455783256192856833"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lgg_pXshA8RF","colab_type":"text"},"source":["### Load the input vectors"]},{"cell_type":"code","metadata":{"id":"NOXi9akbwHZB","colab_type":"code","colab":{}},"source":["with gzip.open('./drive/My Drive/CoE202TermProject/datasets/train.chunk.pickle', 'rb') as f:\n","    traindata = pickle.load(f)\n","\n","train_product   = traindata['product'][:]\n","train_label   = traindata['label'][:]\n","\n","with gzip.open('./drive/My Drive/CoE202TermProject/datasets/valid.chunk.pickle', 'rb') as f:\n","    validdata = pickle.load(f)\n","\n","valid_product   = validdata['product'][:]\n","valid_label   = validdata['label'][:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vlaW3FIsA8RK","colab_type":"code","colab":{}},"source":["batch_size = 512\n","embd_size  = 1024\n","voca_size  = 100001\n","lr         = 1e-4\n","n_epochs   = 3\n","valid_freq = 3\n","max_seq    = len(train_product[0][4])\n","n_classes  = np.max(train_label) + 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XtxIYrsYA8RP","colab_type":"text"},"source":["### Placeholders"]},{"cell_type":"code","metadata":{"id":"6txbjX2tA8RR","colab_type":"code","colab":{}},"source":["products   = tf.placeholder(dtype=tf.float32, shape=[None, max_seq], name='products')\n","product_counts = tf.placeholder(dtype=tf.float32, shape=[None, max_seq], name='product_counts')\n","brands = tf.placeholder(dtype=tf.float32, shape=[None], name='brands')\n","models = tf.placeholder(dtype=tf.float32, shape=[None, max_seq], name='models')\n","model_counts = tf.placeholder(dtype=tf.float32, shape=[None, max_seq], name='model_counts')\n","makers = tf.placeholder(dtype=tf.float32, shape=[None], name='makers')\n","labels   = tf.placeholder(dtype=tf.int32, shape=[None], name='labels')\n","is_train = tf.placeholder(dtype=tf.bool, name='is_train')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZw_gtkeA8RT","colab_type":"text"},"source":["### Deep neural network"]},{"cell_type":"code","metadata":{"id":"5o6qR3E3A8RU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":394},"outputId":"2b19c600-b37d-4c7b-ff6c-659c640960af","executionInfo":{"status":"ok","timestamp":1576757325157,"user_tz":-540,"elapsed":13422,"user":{"displayName":"Khi nenw","photoUrl":"","userId":"08455783256192856833"}}},"source":["# Define embedding\n","embd = Embedding(voca_size, embd_size, name='product_embd')                     # Define embedding function\n","\n","\n","# Product embedding\n","product_embd = embd(products)                                                 # Get the embedding vector of products\n","product_count_shaped = tf.reshape(product_counts, (-1, max_seq, 1))\n","product_embd_mat = tf.keras.layers.dot([product_embd, product_count_shaped], axes=1) # Multiply the counts of products to the corresponding embedding vectors\n","product_embd = tf.reshape(product_embd_mat, (-1, 1, embd_size))\n","product_out = tf.layers.dropout(product_embd, training=is_train)\n","\n","# Brand embedding\n","brands_out = embd(tf.reshape(brands, (-1, 1)))\n","\n","# Model embedding\n","model_embd = embd(models)\n","model_count_shaped = tf.reshape(model_counts, (-1, max_seq, 1))\n","model_embd_mat = tf.keras.layers.dot([model_embd, model_count_shaped], axes=1)\n","model_embd = tf.reshape(model_embd_mat, (-1, 1, embd_size))\n","model_out = tf.layers.dropout(model_embd, training=is_train)\n","\n","# Makers embedidng\n","makers_out = embd(tf.reshape(makers, (-1, 1)))\n","\n","# Combining product and models\n","combined = tf.keras.layers.concatenate([brands_out, model_out, makers_out, product_out], axis=1)\n","bn = tf.layers.batch_normalization(combined, training=is_train)                 # Batch normalization\n","relu = tf.nn.relu(bn)                                                           # ReLU function\n","\n","x = tf.layers.dense(relu, n_classes)\n","x = tf.layers.batch_normalization(x, training=is_train)\n","x = tf.nn.relu(x)\n","x = tf.layers.flatten(x)\n","\n","x = tf.layers.dense(x, n_classes)\n","x = tf.layers.batch_normalization(x, training=is_train)\n","x = tf.nn.relu(x)\n","\n","logits = tf.layers.dense(x, n_classes)\n","\n","# Softmax\n","preds = tf.nn.softmax(logits, name='predictions')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From <ipython-input-6-6bfcfaa45037>:9: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dropout instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From <ipython-input-6-6bfcfaa45037>:26: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n","WARNING:tensorflow:From <ipython-input-6-6bfcfaa45037>:29: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From <ipython-input-6-6bfcfaa45037>:32: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dye2seUzA8RX","colab_type":"text"},"source":["### Loss funciton & Optimizer"]},{"cell_type":"code","metadata":{"id":"SSTWGvtlA8RZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"6d691238-172f-4bac-d654-1f7ec200db97","executionInfo":{"status":"ok","timestamp":1576757325520,"user_tz":-540,"elapsed":13770,"user":{"displayName":"Khi nenw","photoUrl":"","userId":"08455783256192856833"}}},"source":["# Softmax cross entropy loss\n","loss = tf.losses.softmax_cross_entropy(onehot_labels=tf.one_hot(labels, n_classes), logits=logits)\n","\n","# Weight decay\n","reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n","loss = tf.add_n([loss] + reg_losses, name='total_loss')\n","\n","# Optimizer\n","optm = tf.train.AdamOptimizer(lr)\n","train_op = optm.minimize(loss, global_step=tf.train.get_global_step(), name='step_update')\n","update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","train_op = tf.group([train_op, update_ops])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_H7eVZH7G_ij","colab_type":"text"},"source":["### Accuracy"]},{"cell_type":"code","metadata":{"id":"B9PfnmlmG_5w","colab_type":"code","colab":{}},"source":["top1_acc = tf.keras.metrics.top_k_categorical_accuracy(y_true=tf.one_hot(labels, n_classes),\n","                                                        y_pred=preds, k=1)\n","top1_acc = tf.identity(top1_acc, name='top1_acc')\n","\n","top5_acc = tf.keras.metrics.top_k_categorical_accuracy(y_true=tf.one_hot(labels, n_classes),\n","                                                        y_pred=preds, k=5)\n","top5_acc = tf.identity(top5_acc, name='top5_acc')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jkUB1YJpA8Rb","colab_type":"text"},"source":["### Batch generator"]},{"cell_type":"code","metadata":{"id":"-dypzaB9A8Rb","colab_type":"code","colab":{}},"source":["def generator(mode='training'):\n","    if mode == 'training':\n","        n_data = len(train_label)\n","        indices = np.arange(n_data)\n","        np.random.shuffle(indices)\n","        \n","        for start_idx in range(0, n_data, batch_size):\n","            if start_idx + batch_size <= n_data:\n","                excerpt = indices[start_idx: start_idx + batch_size]\n","                yield list(zip(*train_product[excerpt, :])), train_label[excerpt]\n","\n","    elif mode == 'valid':\n","        n_data = len(valid_label)\n","        indices = np.arange(n_data)\n","\n","        for start_idx in range(0, n_data, batch_size):\n","            if start_idx + batch_size <= n_data:\n","                excerpt = indices[start_idx: start_idx + batch_size]\n","                yield list(zip(*valid_product[excerpt, :])), valid_label[excerpt]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"evYsER5OA8Rd","colab_type":"text"},"source":["### Training session"]},{"cell_type":"code","metadata":{"id":"meSSK_xsA8Rd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"outputId":"160d4158-5d52-4d73-ec58-da438c73afaa","executionInfo":{"status":"ok","timestamp":1576757801638,"user_tz":-540,"elapsed":489826,"user":{"displayName":"Khi nenw","photoUrl":"","userId":"08455783256192856833"}}},"source":["import tqdm\n","import math\n","\n","with tf.Session() as sess:\n","    tic = time.time()\n","    saver = tf.train.Saver()\n","    sess.run(tf.global_variables_initializer())\n","    \n","    for epoch in range(n_epochs):\n","        print(\"\\n\\nEpoch {0:03d} / {1:03d}\\n\".format(epoch, n_epochs))\n","        training_loss = []\n","        for b_product, b_label in tqdm.tqdm(\n","            generator(mode='training'),\n","            total=math.ceil(len(train_label) / batch_size)\n","        ):\n","            feed_dict = {products  : b_product[4],\n","                         product_counts: b_product[5],\n","                         brands: b_product[0],\n","                         models: b_product[1],\n","                         model_counts: b_product[2],\n","                         makers: b_product[3],\n","                         labels    : b_label,\n","                         is_train  : True}\n","\n","            _, train_loss = sess.run([train_op, loss], feed_dict=feed_dict)\n","            training_loss.append(train_loss)\n","\n","        toc = time.time()\n","        print(\"[*] TRAIN Loss {0:.4f} | Time {1:.2f}s\".format(np.mean(training_loss), toc - tic))\n","        \n","        if (epoch+1) % valid_freq == 0:\n","            TOP1, TOP5 = [], []\n","            for b_product, b_label in generator(mode='valid'):\n","                feed_dict = {products  : b_product[4],\n","                        product_counts: b_product[5],\n","                        brands: b_product[0],\n","                        models: b_product[1],\n","                        model_counts: b_product[2],\n","                        makers: b_product[3],\n","                        labels    : b_label,\n","                        is_train  : False}\n","                    \n","                t1_acc, t5_acc = sess.run([top1_acc, top5_acc], feed_dict=feed_dict)\n","                TOP1.append(t1_acc)\n","                TOP5.append(t5_acc)\n","            print(\"[*] VALIDATION Top-1 Acc: {0:.4f} | Top-5 Acc: {1:.4f}\".format(np.mean(TOP1), np.mean(TOP5)))\n","\n","    saver.save(sess, './drive/My Drive/CoE202TermProject/models/models')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/1563 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","Epoch 000 / 003\n","\n"],"name":"stdout"},{"output_type":"stream","text":["100%|█████████▉| 1562/1563 [02:32<00:00, 10.25it/s]\n","  0%|          | 1/1563 [00:00<02:58,  8.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["[*] TRAIN Loss 3.4028 | Time 155.26s\n","\n","\n","Epoch 001 / 003\n","\n"],"name":"stdout"},{"output_type":"stream","text":["100%|█████████▉| 1562/1563 [02:33<00:00, 10.16it/s]\n","  0%|          | 1/1563 [00:00<02:58,  8.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["[*] TRAIN Loss 1.3468 | Time 309.03s\n","\n","\n","Epoch 002 / 003\n","\n"],"name":"stdout"},{"output_type":"stream","text":["100%|█████████▉| 1562/1563 [02:33<00:00, 10.18it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["[*] TRAIN Loss 0.7763 | Time 462.53s\n","[*] VALIDATION Top-1 Acc: 0.7541 | Top-5 Acc: 0.9215\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YHpFrEqj4r0L","colab_type":"text"},"source":["## Test process\n","### Load the test data"]},{"cell_type":"code","metadata":{"id":"KwR2OFUesK7C","colab_type":"code","colab":{}},"source":["# test data\n","with gzip.open('./drive/My Drive/CoE202TermProject/datasets/test.chunk.pickle', 'rb') as f:\n","    testdata = pickle.load(f)\n","\n","test_product  = testdata['product'][:]\n","pids          = testdata['pids'][:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b62j6uqe46sJ","colab_type":"text"},"source":["### Batch generator for test dataset"]},{"cell_type":"code","metadata":{"id":"VYyChL_944V-","colab_type":"code","colab":{}},"source":["def generator(mode='test'):\n","    if mode == 'test':\n","        n_data = len(test_product)\n","        indices = np.arange(n_data)\n","        \n","        for start_idx in range(0, n_data, batch_size):\n","            excerpt = indices[start_idx: start_idx + batch_size]\n","            yield list(zip(*test_product[excerpt, :]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pp1YnA1-5AA1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4a6aefee-e06d-4f80-8dc9-e14bcb189496","executionInfo":{"status":"ok","timestamp":1576757810447,"user_tz":-540,"elapsed":498604,"user":{"displayName":"Khi nenw","photoUrl":"","userId":"08455783256192856833"}}},"source":["with tf.Session() as sess:\n","    saver = tf.train.import_meta_graph('./drive/My Drive/CoE202TermProject/models/models.meta')\n","    saver.restore(sess, tf.train.latest_checkpoint('./drive/My Drive/CoE202TermProject/models/'))\n","    DNN = tf.get_default_graph()\n","\n","    preds = []\n","    for b_product in generator(mode='test'):\n","        feed_dict = {DNN.get_tensor_by_name('products:0'): b_product[4],\n","                     DNN.get_tensor_by_name('product_counts:0'): b_product[5],\n","                     DNN.get_tensor_by_name('brands:0'): b_product[0],\n","                     DNN.get_tensor_by_name('models:0'): b_product[1],\n","                     DNN.get_tensor_by_name('model_counts:0'): b_product[2],\n","                     DNN.get_tensor_by_name('makers:0'): b_product[3],\n","                     DNN.get_tensor_by_name('is_train:0')     : False}\n","\n","        pred = sess.run(DNN.get_tensor_by_name('predictions:0'), feed_dict=feed_dict)\n","        preds.extend(pred)\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./drive/My Drive/CoE202TermProject/models/models\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IlrExwdK5CEq","colab_type":"text"},"source":["### Save the submission files"]},{"cell_type":"code","metadata":{"id":"4OXH2IxT5FuX","colab_type":"code","colab":{}},"source":["# Indexing of predictions\n","argpreds = np.argmax(preds, axis=1)\n","\n","# Load label dictionary\n","with open('./drive/My Drive/CoE202TermProject/datasets/y_vocab.pickle', 'rb') as f:\n","    y_dict = pickle.load(f)\n","# y_dict = pickle.loads(open('./drive/My Drive/CoE202TermProject/datasets/y_vocab.pickle').read())\n","\n","# Inverse label dictionary\n","inv_y_dict = dict((y,x) for x,y in y_dict.items())\n","submissions = [inv_y_dict[argpred] for argpred in argpreds]\n","\n","# Write the results to 'submissions.csv'\n","f = open('./drive/My Drive/CoE202TermProject/submissions.csv', 'w')\n","for i, j in zip(pids, submissions):\n","    line = '{},{}\\n'.format(i,j)\n","    f.write(line)\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vwd53pai5i3i","colab_type":"text"},"source":["You should submit the 'submissions.csv' file and 4 tf.save files ('checkpoint', 'dnn_models.data', 'dnn_models.index', 'dnn_models.meta') in models folder"]}]}