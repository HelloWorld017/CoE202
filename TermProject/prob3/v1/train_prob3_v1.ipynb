{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "train_prob3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auZnofLD9q-t",
        "colab_type": "text"
      },
      "source": [
        "## Initialize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYXri2279swj",
        "colab_type": "text"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1Xr_8TXA8Q9",
        "colab_type": "code",
        "outputId": "d0197384-d37e-484a-d1c9-ed884078d83d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "from time import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2zMmlT-98Yh",
        "colab_type": "text"
      },
      "source": [
        "### Defining Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpLYY1WR9-P5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = '.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJpDoVUH9us3",
        "colab_type": "text"
      },
      "source": [
        "### Mounting File System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg4QjiBNBMgH",
        "colab_type": "code",
        "outputId": "ba2b235f-5c79-403d-857c-46296d85bb53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = './drive/My Drive/CoE202TermProject'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgg_pXshA8RF",
        "colab_type": "text"
      },
      "source": [
        "### Loading Input Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOXi9akbwHZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with gzip.open(base_path + '/datasets/train.chunk.pickle', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "\n",
        "with gzip.open(base_path + '/datasets/train.image.feats.pickle', 'rb') as f:\n",
        "    train_images = pickle.load(f)\n",
        "\n",
        "train_features = train_data['features'][:]\n",
        "train_labels = train_data['labels'][:]\n",
        "\n",
        "\n",
        "with gzip.open(base_path + '/datasets/valid.chunk.pickle', 'rb') as f:\n",
        "    valid_data = pickle.load(f)\n",
        "\n",
        "with gzip.open(base_path + '/datasets/valid.image.feats.pickle', 'rb') as f:\n",
        "    valid_images = pickle.load(f)\n",
        "\n",
        "valid_features = valid_data['features'][:]\n",
        "valid_labels = valid_data['labels'][:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2EedvGE-Xq1",
        "colab_type": "text"
      },
      "source": [
        "## Defining Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5gMGUXo-dbE",
        "colab_type": "text"
      },
      "source": [
        "### Defining Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlaW3FIsA8RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 512\n",
        "lr = 1e-4\n",
        "n_epochs = 3\n",
        "gru_units = 256\n",
        "embd_size  = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAx0-U1y-jrN",
        "colab_type": "text"
      },
      "source": [
        "### Defining Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm5PktEY-lGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voca_size  = 100001\n",
        "valid_freq = 3\n",
        "seqlen_model = len(train_features[0][1])\n",
        "seqlen_product = len(train_features[0][3])\n",
        "seqlen_image = len(train_images[0])\n",
        "n_classes  = np.max(train_labels) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtxIYrsYA8RP",
        "colab_type": "text"
      },
      "source": [
        "### Placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6txbjX2tA8RR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_brands = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, ), name='brands'\n",
        ")\n",
        "\n",
        "input_models = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, seqlen_model), name='input_models'\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "input_model_counts = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, seqlen_model), name='input_model_counts'\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "input_makers = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, ), name='makers'\n",
        ")\n",
        "\n",
        "input_products = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, seqlen_product), name='input_products'\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "input_product_counts = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, seqlen_product), name='input_product_counts'\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "input_prices = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, ), name='input_prices'\n",
        ")\n",
        "\n",
        "input_images = tf.placeholder(\n",
        "    dtype=tf.float32, shape=(None, 2048), name='input_images'\n",
        ")\n",
        "\n",
        "labels = tf.placeholder(\n",
        "    dtype=tf.int32, shape=(None, ), name='labels'\n",
        ")\n",
        "\n",
        "is_train = tf.placeholder(\n",
        "    dtype=tf.bool, name='is_train'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZw_gtkeA8RT",
        "colab_type": "text"
      },
      "source": [
        "### Deep neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o6qR3E3A8RU",
        "colab_type": "code",
        "outputId": "4738bd32-5145-4972-d649-7a97fedeff62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "from tensorflow.keras.layers import Activation, BatchNormalization, \\\n",
        "                                    Bidirectional, Concatenate, Dense, \\\n",
        "                                    Embedding, Flatten, GRU, Reshape\n",
        "\n",
        "# Defining Embedding Layer\n",
        "embd = Embedding(voca_size, embd_size, name='embd')\n",
        "\n",
        "# Processing Brands\n",
        "x_brands = Reshape((1, ))(input_brands)\n",
        "x_brands = embd(x_brands)\n",
        "x_brands = Dense(256)(x_brands)\n",
        "x_brands = BatchNormalization()(x_brands, training=is_train)\n",
        "x_brands = Activation('relu')(x_brands)\n",
        "x_brands = Flatten()(x_brands)\n",
        "\n",
        "# Processing Models\n",
        "x_models = embd(input_models)\n",
        "x_models = Bidirectional(GRU(\n",
        "    gru_units, recurrent_activation='sigmoid', reset_after=True\n",
        "))(x_models)\n",
        "\n",
        "# Processing Makers\n",
        "x_makers = Reshape((1, ))(input_makers)\n",
        "x_makers = embd(x_makers)\n",
        "x_makers = Dense(256)(x_makers)\n",
        "x_makers = BatchNormalization()(x_makers, training=is_train)\n",
        "x_makers = Activation('relu')(x_makers)\n",
        "x_makers = Flatten()(x_makers)\n",
        "\n",
        "# Processing Products\n",
        "x_products = embd(input_products)\n",
        "x_products = Bidirectional(GRU(\n",
        "    gru_units, recurrent_activation='sigmoid', reset_after=True\n",
        "))(x_products)\n",
        "\n",
        "# Processing Long Texts\n",
        "long_text_concat = Concatenate(axis=1)([x_models, x_products])\n",
        "long_text_hidden = Dense(1024)(long_text_concat)\n",
        "long_text_hidden = BatchNormalization()(long_text_hidden, training=is_train)\n",
        "long_text_hidden = Activation('relu')(long_text_hidden)\n",
        "\n",
        "# Processing Texts\n",
        "text_concat = Concatenate(axis=1)([x_brands, x_makers, long_text_hidden])\n",
        "text_flatten = Flatten()(text_concat)\n",
        "text_hidden = Dense(n_classes)(long_text_concat)\n",
        "text_hidden = BatchNormalization()(text_hidden, training=is_train)\n",
        "text_hidden = Activation('relu')(text_hidden)\n",
        "\n",
        "# Processing Images\n",
        "x_images = Dense(n_classes * 2)(input_images)\n",
        "x_images = BatchNormalization()(x_images, training=is_train)\n",
        "x_images = Activation('relu')(x_images)\n",
        "\n",
        "image_hidden = Dense(n_classes)(x_images)\n",
        "image_hidden = BatchNormalization()(image_hidden, training=is_train)\n",
        "image_hidden = Activation('relu')(image_hidden)\n",
        "\n",
        "# Normalizing Prices\n",
        "x_prices = Reshape((1, ))(input_prices)\n",
        "x_prices = BatchNormalization()(x_prices, training=is_train)\n",
        "\n",
        "# Concatenating Images and Prices\n",
        "x_concat = Concatenate(axis=1)([x_products, x_images, x_prices])\n",
        "\n",
        "# Hidden Layer\n",
        "hidden1 = Dense(n_classes)(x_concat)\n",
        "hidden1 = BatchNormalization()(hidden1, training=is_train)\n",
        "hidden1 = Activation('relu')(hidden1)\n",
        "\n",
        "# Output\n",
        "logits = Dense(n_classes)(hidden1)\n",
        "\n",
        "# Softmax\n",
        "preds = Activation('softmax', name='predictions')(logits)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dye2seUzA8RX",
        "colab_type": "text"
      },
      "source": [
        "### Loss funciton & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSTWGvtlA8RZ",
        "colab_type": "code",
        "outputId": "724591e7-27c3-4681-c7aa-95bdfb283701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Softmax cross entropy loss\n",
        "loss = tf.losses.softmax_cross_entropy(onehot_labels=tf.one_hot(labels, n_classes), logits=logits)\n",
        "\n",
        "# Weight decay\n",
        "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "loss = tf.add_n([loss] + reg_losses, name='total_loss')\n",
        "\n",
        "# Optimizer\n",
        "optm = tf.train.AdamOptimizer(lr)\n",
        "train_op = optm.minimize(loss, global_step=tf.train.get_global_step(), name='step_update')\n",
        "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "train_op = tf.group([train_op, update_ops])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H7eVZH7G_ij",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9PfnmlmG_5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top1_acc = tf.keras.metrics.top_k_categorical_accuracy(y_true=tf.one_hot(labels, n_classes),\n",
        "                                                        y_pred=preds, k=1)\n",
        "top1_acc = tf.identity(top1_acc, name='top1_acc')\n",
        "\n",
        "top5_acc = tf.keras.metrics.top_k_categorical_accuracy(y_true=tf.one_hot(labels, n_classes),\n",
        "                                                        y_pred=preds, k=5)\n",
        "top5_acc = tf.identity(top5_acc, name='top5_acc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkUB1YJpA8Rb",
        "colab_type": "text"
      },
      "source": [
        "### Batch generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dypzaB9A8Rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(mode='training'):\n",
        "    if mode == 'training':\n",
        "        features = train_features\n",
        "        labels = train_labels\n",
        "        images = train_images\n",
        "\n",
        "    elif mode == 'valid':\n",
        "        features = valid_features\n",
        "        labels = valid_labels\n",
        "        images = valid_images\n",
        "    \n",
        "    elif mode == 'test':\n",
        "        features = test_features\n",
        "        labels = None\n",
        "        images = test_images\n",
        "\n",
        "    n_data = len(features)\n",
        "    indices = np.arange(n_data)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start_idx in range(0, n_data, batch_size):\n",
        "        if start_idx + batch_size <= n_data:\n",
        "            excerpt = indices[start_idx: start_idx + batch_size]\n",
        "\n",
        "            if labels is not None:\n",
        "                yield list(zip(*features[excerpt, :])), \\\n",
        "                    images[excerpt, :], \\\n",
        "                    labels[excerpt]\n",
        "            \n",
        "            else:\n",
        "                yield list(zip(*features[excerpt, :])), \\\n",
        "                    images[excerpt, :]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evYsER5OA8Rd",
        "colab_type": "text"
      },
      "source": [
        "### Training session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meSSK_xsA8Rd",
        "colab_type": "code",
        "outputId": "e385a8bc-6470-41c3-fc00-6a7df70b33ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "import tqdm\n",
        "import math\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    tic = time()\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        print(\n",
        "            \"\\n\\nEpoch {0:03d} / {1:03d}\\n\"\n",
        "            .format(epoch, n_epochs)\n",
        "        )\n",
        "\n",
        "        training_loss = []\n",
        "\n",
        "        for b_features, b_images, b_label in tqdm.tqdm(\n",
        "            generator(mode = 'training'),\n",
        "            total = math.ceil(len(train_labels) / batch_size)\n",
        "        ):\n",
        "            feed_dict = {\n",
        "                input_brands: b_features[0],\n",
        "                input_models: b_features[1],\n",
        "                input_makers: b_features[2],\n",
        "                input_products: b_features[3],\n",
        "                input_prices: b_features[4],\n",
        "                input_images: b_images,\n",
        "                labels: b_label,\n",
        "                is_train: True\n",
        "            }\n",
        "            \n",
        "            _, train_loss = sess.run(\n",
        "                [train_op, loss],\n",
        "                feed_dict=feed_dict\n",
        "            )\n",
        "\n",
        "            training_loss.append(train_loss)\n",
        "\n",
        "        toc = time()\n",
        "        print(\n",
        "            \"[*] TRAIN Loss {0:.4f} | Time {1:.2f}s\"\n",
        "            .format(np.mean(training_loss), toc - tic)\n",
        "        )\n",
        "        \n",
        "        if (epoch + 1) % valid_freq == 0:\n",
        "            top_1, top_5 = [], []\n",
        "\n",
        "            for b_features, b_images, b_label in generator(mode = 'valid'):\n",
        "                feed_dict = {\n",
        "                    input_brands: b_features[0],\n",
        "                    input_models: b_features[1],\n",
        "                    input_makers: b_features[2],\n",
        "                    input_products: b_features[3],\n",
        "                    input_prices: b_features[4],\n",
        "                    input_images: b_images,\n",
        "                    labels: b_label,\n",
        "                    is_train: False\n",
        "                }\n",
        "                    \n",
        "                t1_acc, t5_acc = sess.run(\n",
        "                    [top1_acc, top5_acc],\n",
        "                    feed_dict=feed_dict\n",
        "                )\n",
        "\n",
        "                top_1.append(t1_acc)\n",
        "                top_5.append(t5_acc)\n",
        "\n",
        "            print(\n",
        "                \"[*] VALIDATION Top-1 Acc: {0:.4f} | Top-5 Acc: {1:.4f}\"\n",
        "                .format(np.mean(top_1), np.mean(top_5))\n",
        "            )\n",
        "\n",
        "    saver.save(sess, './drive/My Drive/CoE202TermProject/models/models')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1563 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch 000 / 003\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1562/1563 [03:29<00:00,  7.55it/s]\n",
            "  0%|          | 1/1563 [00:00<03:57,  6.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[*] TRAIN Loss 1.9153 | Time 218.75s\n",
            "\n",
            "\n",
            "Epoch 001 / 003\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1562/1563 [03:26<00:00,  7.61it/s]\n",
            "  0%|          | 1/1563 [00:00<03:55,  6.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[*] TRAIN Loss 0.8257 | Time 425.24s\n",
            "\n",
            "\n",
            "Epoch 002 / 003\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1562/1563 [03:26<00:00,  7.56it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[*] TRAIN Loss 0.5518 | Time 631.90s\n",
            "[*] VALIDATION Top-1 Acc: 0.3179 | Top-5 Acc: 0.8401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHpFrEqj4r0L",
        "colab_type": "text"
      },
      "source": [
        "## Test process\n",
        "### Load the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwR2OFUesK7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test data\n",
        "with gzip.open('./drive/My Drive/CoE202TermProject/datasets/test.chunk.pickle', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "with gzip.open('./drive/My Drive/CoE202TermProject/datasets/valid.image.feats.pickle', 'rb') as f:\n",
        "    test_images = pickle.load(f)\n",
        "\n",
        "test_features  = test_data['features'][:]\n",
        "pids = test_data['pids'][:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b62j6uqe46sJ",
        "colab_type": "text"
      },
      "source": [
        "### Batch generator for test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp1YnA1-5AA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    saver = tf.train.import_meta_graph('./drive/My Drive/CoE202TermProject/models/models.meta')\n",
        "    saver.restore(sess, tf.train.latest_checkpoint('./drive/My Drive/CoE202TermProject/models/'))\n",
        "    DNN = tf.get_default_graph()\n",
        "\n",
        "    preds = []\n",
        "    for b_features, b_images in generator(mode='test'):\n",
        "        feed_dict = {\n",
        "            DNN.get_tensor_by_name('input_brands:0'): b_features[0],\n",
        "            DNN.get_tensor_by_name('input_models:0'): b_features[1],\n",
        "            DNN.get_tensor_by_name('input_makers:0'): b_features[2],\n",
        "            DNN.get_tensor_by_name('input_products:0'): b_features[3],\n",
        "            DNN.get_tensor_by_name('input_prices:0'): b_features[4],\n",
        "            DNN.get_tensor_by_name('input_images:0'): b_images,\n",
        "            DNN.get_tensor_by_name('is_train:0'): False\n",
        "        }\n",
        "\n",
        "        pred = sess.run(DNN.get_tensor_by_name('predictions:0'), feed_dict=feed_dict)\n",
        "        preds.extend(pred)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlrExwdK5CEq",
        "colab_type": "text"
      },
      "source": [
        "### Save the submission files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OXH2IxT5FuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Indexing of predictions\n",
        "argpreds = np.argmax(preds, axis=1)\n",
        "\n",
        "# Load label dictionary\n",
        "with open('./drive/My Drive/CoE202TermProject/datasets/y_vocab.pickle', 'rb') as f:\n",
        "    y_dict = pickle.load(f)\n",
        "# y_dict = pickle.loads(open('./drive/My Drive/CoE202TermProject/datasets/y_vocab.pickle').read())\n",
        "\n",
        "# Inverse label dictionary\n",
        "inv_y_dict = dict((y,x) for x,y in y_dict.items())\n",
        "submissions = [inv_y_dict[argpred] for argpred in argpreds]\n",
        "\n",
        "# Write the results to 'submissions.csv'\n",
        "f = open('./drive/My Drive/CoE202TermProject/submissions.csv', 'w')\n",
        "for i, j in zip(pids, submissions):\n",
        "    line = '{},{}\\n'.format(i,j)\n",
        "    f.write(line)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwd53pai5i3i",
        "colab_type": "text"
      },
      "source": [
        "You should submit the 'submissions.csv' file and 4 tf.save files ('checkpoint', 'dnn_models.data', 'dnn_models.index', 'dnn_models.meta') in models folder"
      ]
    }
  ]
}